import hashlib
import os
import json
import datetime

# Define the hashing algorithm to use
HASH_ALGORITHM = 'sha256'
# Default name for the hash database file
DEFAULT_DB_FILE = 'file_hashes.json'

def calculate_file_hash(filepath: str) -> str | None:
    """
    Calculates the SHA256 hash of a given file.

    Args:
        filepath (str): The path to the file.

    Returns:
        str | None: The hexadecimal digest of the file's hash, or None if an error occurs.
    """
    try:
        # Use a buffer to read large files efficiently
        hasher = hashlib.new(HASH_ALGORITHM)
        with open(filepath, 'rb') as f:
            while chunk := f.read(8192):  # Read in 8KB chunks
                hasher.update(chunk)
        return hasher.hexdigest()
    except FileNotFoundError:
        print(f"Error: File not found - {filepath}")
        return None
    except IOError as e:
        print(f"Error reading file {filepath}: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred while hashing {filepath}: {e}")
        return None

def scan_directory_and_store_hashes(directory_path: str, db_file: str = DEFAULT_DB_FILE):
    """
    Scans a directory, calculates hashes for all files, and stores them in a JSON database.

    Args:
        directory_path (str): The path to the directory to monitor.
        db_file (str): The name of the file to store the hash database.
    """
    if not os.path.isdir(directory_path):
        print(f"Error: Directory not found - {directory_path}")
        return

    print(f"Scanning directory: {directory_path}...")
    file_hashes = {}
    skipped_count = 0

    # Walk through the directory tree
    for root, _, files in os.walk(directory_path):
        for filename in files:
            filepath = os.path.join(root, filename)
            # Get relative path for consistent storage, independent of where the script runs
            relative_filepath = os.path.relpath(filepath, directory_path)
            file_hash = calculate_file_hash(filepath)

            if file_hash:
                file_hashes[relative_filepath] = file_hash
            else:
                skipped_count += 1
                # The error message is already printed by calculate_file_hash

    try:
        # Store the hashes along with metadata (scan time, monitored directory)
        db_content = {
            'metadata': {
                'scan_time': datetime.datetime.now().isoformat(),
                'monitored_directory': os.path.abspath(directory_path),
                'hash_algorithm': HASH_ALGORITHM
            },
            'files': file_hashes
        }
        with open(db_file, 'w') as f:
            json.dump(db_content, f, indent=4)
        print(f"\nScan complete. Hashes for {len(file_hashes)} files stored in '{db_file}'.")
        if skipped_count > 0:
            print(f"Warning: Skipped {skipped_count} files due to errors.")
    except IOError as e:
        print(f"Error saving hash database to {db_file}: {e}")
    except Exception as e:
        print(f"An unexpected error occurred while saving the database: {e}")


def verify_file_integrity(directory_path: str, db_file: str = DEFAULT_DB_FILE):
    """
    Verifies the integrity of files in a directory by comparing current hashes with stored ones.

    Args:
        directory_path (str): The path to the directory to verify.
        db_file (str): The path to the stored hash database file.
    """
    if not os.path.isdir(directory_path):
        print(f"Error: Directory not found - {directory_path}")
        return

    if not os.path.exists(db_file):
        print(f"Error: Hash database file '{db_file}' not found. Please run a scan first.")
        return

    print(f"Verifying integrity of directory: {directory_path} using '{db_file}'...")

    try:
        with open(db_file, 'r') as f:
            db_content = json.load(f)
            stored_hashes = db_content.get('files', {})
            metadata = db_content.get('metadata', {})

        print(f"\n--- Previous Scan Details ---")
        print(f"Monitored Directory: {metadata.get('monitored_directory', 'N/A')}")
        print(f"Scan Time: {metadata.get('scan_time', 'N/A')}")
        print(f"Hash Algorithm: {metadata.get('hash_algorithm', 'N/A')}")
        print(f"---------------------------\n")

        current_files = set()
        modified_files = []
        new_files = []
        unchanged_count = 0
        current_scan_skipped_count = 0

        # Create a set of all current files with their relative paths
        for root, _, files in os.walk(directory_path):
            for filename in files:
                filepath = os.path.join(root, filename)
                relative_filepath = os.path.relpath(filepath, directory_path)
                current_files.add(relative_filepath)

                # Calculate current hash and compare with stored hash
                if relative_filepath in stored_hashes:
                    current_hash = calculate_file_hash(filepath)
                    if current_hash is None:
                        current_scan_skipped_count += 1
                        continue # Error already printed by calculate_file_hash
                    
                    if stored_hashes[relative_filepath] != current_hash:
                        modified_files.append(relative_filepath)
                    else:
                        unchanged_count += 1
                else:
                    new_files.append(relative_filepath)

        # Identify deleted files
        deleted_files = [
            f for f in stored_hashes if f not in current_files
        ]

        # Report findings
        print("--- Integrity Check Results ---")
        if modified_files:
            print("\nðŸš¨ MODIFIED FILES (Hash Mismatch):")
            for f in modified_files:
                print(f"  - {f}")
        else:
            print("\nâœ… No modified files detected.")

        if new_files:
            print("\nâž• NEW FILES (Not in previous scan):")
            for f in new_files:
                print(f"  - {f}")
        else:
            print("\nâœ… No new files detected.")

        if deleted_files:
            print("\nâž– DELETED FILES (Missing since previous scan):")
            for f in deleted_files:
                print(f"  - {f}")
        else:
            print("\nâœ… No deleted files detected.")

        print(f"\nSummary:")
        print(f"  - Files checked: {len(current_files)}")
        print(f"  - Unchanged files: {unchanged_count}")
        print(f"  - Modified files: {len(modified_files)}")
        print(f"  - New files: {len(new_files)}")
        print(f"  - Deleted files: {len(deleted_files)}")
        if current_scan_skipped_count > 0:
            print(f"  - Files skipped during current scan: {current_scan_skipped_count} (due to errors)")

    except json.JSONDecodeError:
        print(f"Error: Could not parse '{db_file}'. Ensure it's a valid JSON file.")
    except IOError as e:
        print(f"Error reading hash database from {db_file}: {e}")
    except Exception as e:
        print(f"An unexpected error occurred during verification: {e}")

def main():
    """
    Main function to provide a command-line interface for the file integrity checker.
    """
    print("\n--- File Integrity Checker ---")
    print("This tool monitors changes in files by calculating and comparing hash values.")

    monitored_directory = input("Enter the directory path to monitor/verify (e.g., ./my_project): ").strip()
    if not monitored_directory:
        print("Directory path cannot be empty. Exiting.")
        return

    # Use a specific database file name derived from the directory for clarity
    db_filename = os.path.basename(os.path.normpath(monitored_directory)) + "_hashes.json"
    print(f"Using database file: {db_filename}")

    while True:
        print("\nChoose an option:")
        print("1. Scan directory and create/update hash database")
        print("2. Verify file integrity against the hash database")
        print("3. Exit")
        choice = input("Enter your choice (1/2/3): ").strip()

        if choice == '1':
            scan_directory_and_store_hashes(monitored_directory, db_filename)
        elif choice == '2':
            verify_file_integrity(monitored_directory, db_filename)
        elif choice == '3':
            print("Exiting File Integrity Checker. Goodbye!")
            break
        else:
            print("Invalid choice. Please enter 1, 2, or 3.")

if __name__ == "__main__":
    main()
